# 基础函数功能解读

## Spark定义的基本transformation和action的操作

| 类型 | 函数 | 结果 |
| --- | --- | --- |
| Transformactions | map(f:T->U) | RDD[T]->RDD[U] |
| Transformactions | filter(f:T->Bool) | RDD[T]->RDD[T] |
| Transformactions | flatMap(f:T->Seq[U]) | RDD[T]->RDD[U] |
| Transformactions | sample(fraction:Float) | RDD[T]->RDD[T]（确定性采样）(Deterministic sampling) |
| Transformactions | groupByKey() | RDD[(K,V)]->RDD[(K,Seq[V])] |
| Transformactions | reductByKey(f:(V,V)->V) | RDD[(K,V)]->RDD[(K,V)] |
| Transformactions | union() | (RDD[T],RDD[T])->RDD[T] |
| Transformactions | join() | (RDD[(K,V)],RDD[(K,W)])->RDD[(K,(V,W))] |
| Transformactions | cogroup() | (RDD[(K,V)],RDD[(K,W)])->RDD[(K,(Seq[V],Seq[W]))] |
| Transformactions | crossProduct() | (RDD[T],RDD[T])->RDD[T,U] |
| Transformactions | mapValues(f:V->W) | RDD[K,V]->RDD[K,W] （保护性划分）(Preserves partitioning)|
| Transformactions | sort(c:Comparator[K]) | RDD[K,V]->RDD[K,V] |
| Transformactions | partitionBy(p:Partitioner[K]) | RDD[(K,V)]->RDD[(K,V)] |
| Actions | count() | RDD[T]->Long |
| Actions | collect() | RDD[T]->Seq[T] |
| Actions | reduce(f:(T,T)->T) | RDD[T]->T |
| Actions | lookup(k:K) | RDD[(K,V)]->Seq[V]（依赖哈希或范围算法的RDD分区）(On hash/range partitioned RDDs) |
| Actions | save(path:String) | Outputs RDD to a storage system, e.g.HDFS |

## 实例开发

```
val rdd = sc.parallelize(List(1,2,3,4,5,6))  
val mapRdd = rdd.map(_*2)  //这是典型的函数式编程
mapRdd.collect()  //上面的map是transformation，到了这里的collect才开始执行，是action，返回一个Array    Array(2,4,6,8,10,12)

val filterRdd = mapRdd.filter(_ > 5)
filterRdd.collect() //返回所有大于5的数据的一个Array， Array(6,8,10,12)

val rdd = sc.textFile("/xxx/sss/ee")
rdd.count //计算行数
rdd.cache   //可以把rdd保留在内存里面
rdd.count //计算行数，但是因为上面进行了cache，这里速度会很快

val wordcount = rdd.flatMap(_.split(' ')).map((_, 1)).reduceByKey(_+_)  //把每一行进行根据空格分割，然后flatMap会把多个list合并成一个list，最后把每个元素变成一个元组
//然后把具有相同key的元素的value进行相加操作，参考上面图片中的函数定义，针对reduceByKey，传入的函数是对value进行操作的。
wordcount.saveAsTextFile("/xxx/ss/aa")   //把结果存入文件系统
wordcount.collect //可以得到一个数组

val rdd1 = sc.parallelize(List(('a',1),(‘a’, 2)))
val rdd2 = sc.parallelize(List(('b',1),(‘b’, 2)))
val result_union = rdd1 union rdd2 //结果是把两个list合并成一个，List(('a',1),(‘a’, 2),('b',1),(‘b’, 2))

val rdd1 = sc.parallelize(List(('a',1),(‘a’, 2), ('b', 3)))
val rdd2 = sc.parallelize(List(('a',4),(‘b’, 5)))
val result_union = rdd1 join rdd2 //结果是把两个list做笛卡尔积，Array(('a', (1,4), ('a', (2,4), ('b', (3, 5)))

val rdd = sc.parallelize(List(1,2,3,4))
rdd.reduce(_+_) //reduce是一个action，这里的结果是10

val rdd = sc.parallelize(List(('a',1),(‘a’, 2),('b',1),(‘b’, 2))
rdd.lookup("a") //返回一个seq， (1, 2) 是把a对应的所有元素的value提出来组成一个seq

val wordcount = rdd.flatMap(_split(' ')).map(_,1).reduceByKey(_+_).map(x => (x._2, x._1)).sortByKey(false).map(x => (x._2, x._1))
//其实完成了一个sort by value的过程， sortByKey(false)，表示倒序排列
```

## 参考

* [SparkRDDAPIExamples](http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html)